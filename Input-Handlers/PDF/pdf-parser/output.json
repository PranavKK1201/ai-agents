{"root": {"content": ["\n", "Jonathan Ho UC Berkeley jonathanho@berkeley.edu\n", "\n", "Ajay Jain UC Berkeley ajayj@berkeley.edu\n", "\n", "Pieter Abbeel UC Berkeley pabbeel@cs.berkeley.edu\n", "\n", "\n", "We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion.\n", "\n"], "children": [{"1 Introduction": {"content": ["\n", "Deep generative models of all kinds have recently exhibited high quality samples in a wide variety of data modalities. Generative adversarial networks (GANs), autoregressive models, flows, and variational autoencoders (VAEs) have synthesized striking image and audio samples [14, 27, 3, 58, 38, 25, 10, 32, 44, 57, 26, 33, 45], and there have been remarkable advances in energy-based modeling and score matching that have produced images comparable to those of GANs [11, 55].\n", "\n", "![](_page_0_Picture_8.jpeg)\n", "\n", "Figure 1: Generated samples on CelebA-HQ 256 \u00d7 256 (left) and unconditional CIFAR10 (right)\n", "\n", "34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\n", "\n", "![](_page_1_Figure_0.jpeg)\n", "\n", "Figure 2: The directed graphical model considered in this work.\n", "\n", "This paper presents progress in diffusion probabilistic models [53]. A diffusion probabilistic model (which we will call a \"diffusion model\" for brevity) is a parameterized Markov chain trained using variational inference to produce samples matching the data after finite time. Transitions of this chain are learned to reverse a diffusion process, which is a Markov chain that gradually adds noise to the data in the opposite direction of sampling until signal is destroyed. When the diffusion consists of small amounts of Gaussian noise, it is sufficient to set the sampling chain transitions to conditional Gaussians too, allowing for a particularly simple neural network parameterization.\n", "\n", "Diffusion models are straightforward to define and efficient to train, but to the best of our knowledge, there has been no demonstration that they are capable of generating high quality samples. We show that diffusion models actually are capable of generating high quality samples, sometimes better than the published results on other types of generative models (Section 4). In addition, we show that a certain parameterization of diffusion models reveals an equivalence with denoising score matching over multiple noise levels during training and with annealed Langevin dynamics during sampling (Section 3.2) [55, 61]. We obtained our best sample quality results using this parameterization (Section 4.2), so we consider this equivalence to be one of our primary contributions.\n", "\n", "Despite their sample quality, our models do not have competitive log likelihoods compared to other likelihood-based models (our models do, however, have log likelihoods better than the large estimates annealed importance sampling has been reported to produce for energy based models and score matching [11, 55]). We find that the majority of our models' lossless codelengths are consumed to describe imperceptible image details (Section 4.3). We present a more refined analysis of this phenomenon in the language of lossy compression, and we show that the sampling procedure of diffusion models is a type of progressive decoding that resembles autoregressive decoding along a bit ordering that vastly generalizes what is normally possible with autoregressive models.\n", "\n"], "children": []}}, {"2 Background": {"content": ["\n", "Diffusion models [53] are latent variable models of the form p\u03b8(x0) := R p\u03b8(x0:T ) dx1:T , where x1, . . . , xT are latents of the same dimensionality as the data x0 \u223c q(x0). The joint distribution p\u03b8(x0:T ) is called the *reverse process*, and it is defined as a Markov chain with learned Gaussian transitions starting at p(xT ) = N (xT ; 0, I):\n", "\n", "$$p_{\\theta}(\\mathbf{x}_{0:T}):=p(\\mathbf{x}_{T})\\prod_{t=1}^{T}p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_{t}),\\qquad p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_{t}):=\\mathcal{N}(\\mathbf{x}_{t-1};\\boldsymbol{\\mu}_{\\theta}(\\mathbf{x}_{t},t),\\boldsymbol{\\Sigma}_{\\theta}(\\mathbf{x}_{t},t))\\tag{1}$$\n", "\n", "What distinguishes diffusion models from other types of latent variable models is that the approximate posterior q(x1:T |x0), called the *forward process* or *diffusion process*, is fixed to a Markov chain that gradually adds Gaussian noise to the data according to a variance schedule \u03b21, . . . , \u03b2T :\n", "\n", "$$q(\\mathbf{x}_{1:T}|\\mathbf{x}_{0}):=\\prod_{t=1}^{T}q(\\mathbf{x}_{t}|\\mathbf{x}_{t-1}),\\qquad q(\\mathbf{x}_{t}|\\mathbf{x}_{t-1}):=\\mathcal{N}(\\mathbf{x}_{t};\\sqrt{1-\\beta_{t}}\\mathbf{x}_{t-1},\\beta_{t}\\mathbf{I})\\tag{2}$$\n", "\n", "Training is performed by optimizing the usual variational bound on negative log likelihood:\n", "\n", "$$\\mathbb{E}\\left[-\\log p_{\\theta}(\\mathbf{x}_{0})\\right]\\leq\\mathbb{E}_{q}\\left[-\\log\\frac{p_{\\theta}(\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T}|\\mathbf{x}_{0})}\\right]=\\mathbb{E}_{q}\\left[-\\log p(\\mathbf{x}_{T})-\\sum_{t\\geq1}\\log\\frac{p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_{t})}{q(\\mathbf{x}_{t}|\\mathbf{x}_{t-1})}\\right]=:L\\tag{3}$$\n", "\n", "The forward process variances \u03b2t can be learned by reparameterization [33] or held constant as hyperparameters, and expressiveness of the reverse process is ensured in part by the choice of Gaussian conditionals in p\u03b8(xt\u22121|xt), because both processes have the same functional form when \u03b2t are small [53]. A notable property of the forward process is that it admits sampling xt at an arbitrary timestep t in closed form: using the notation \u03b1t := 1 \u2212 \u03b2t and \u03b1\u00aft := Qt s=1 \u03b1s, we have\n", "\n", "$$q({\\bf x}_{t}|{\\bf x}_{0})={\\cal N}({\\bf x}_{t};\\sqrt{\\alpha_{t}}{\\bf x}_{0},(1-\\bar{\\alpha}_{t}){\\bf I})\\tag{4}$$\n", "\n", "Efficient training is therefore possible by optimizing random terms of L with stochastic gradient descent. Further improvements come from variance reduction by rewriting L (3) as:\n", "\n", "$$\\mathbb{E}_{q}\\left[\\underbrace{D_{\\text{KL}}(q(\\mathbf{x}_{T}|\\mathbf{x}_{0})\\parallel p(\\mathbf{x}_{T}))}_{L_{T}}+\\sum_{t>1}D_{\\text{KL}}(q(\\mathbf{x}_{t-1}|\\mathbf{x}_{t},\\mathbf{x}_{0})\\parallel p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_{t}))\\underbrace{-\\log p_{\\theta}(\\mathbf{x}_{0}|\\mathbf{x}_{1})}_{L_{t-1}}\\right]\\tag{5}$$\n", "\n", "(See Appendix A for details. The labels on the terms are used in Section 3.) Equation (5) uses KL divergence to directly compare p\u03b8(xt\u22121|xt) against forward process posteriors, which are tractable when conditioned on x0:\n", "\n", "$$q({\\bf x}_{t-1}|{\\bf x}_{t},{\\bf x}_{0})={\\cal N}({\\bf x}_{t-1};\\,\\tilde{\\mu}_{t}({\\bf x}_{t},{\\bf x}_{0}),\\tilde{\\beta}_{t}{\\bf I}),\\tag{6}$$\n", "\n", "where $\\tilde{\\mu}_{t}(\\mathbf{x}_{t},\\mathbf{x}_{0}):=\\frac{\\sqrt{\\alpha_{t-1}}\\beta_{t}}{1-\\tilde{\\alpha}_{t}}\\mathbf{x}_{0}+\\frac{\\sqrt{\\alpha_{t}}(1-\\tilde{\\alpha}_{t-1})}{1-\\tilde{\\alpha}_{t}}\\mathbf{x}_{t}\\quad\\text{and}\\quad\\tilde{\\beta}_{t}:=\\frac{1-\\tilde{\\alpha}_{t-1}}{1-\\tilde{\\alpha}_{t}}\\beta_{t}$ (7)\n", "\n", "Consequently, all KL divergences in Eq. (5) are comparisons between Gaussians, so they can be calculated in a Rao-Blackwellized fashion with closed form expressions instead of high variance Monte Carlo estimates.\n", "\n"], "children": []}}, {"3 Diffusion models and denoising autoencoders": {"content": ["\n", "Diffusion models might appear to be a restricted class of latent variable models, but they allow a large number of degrees of freedom in implementation. One must choose the variances \u03b2t of the forward process and the model architecture and Gaussian distribution parameterization of the reverse process. To guide our choices, we establish a new explicit connection between diffusion models and denoising score matching (Section 3.2) that leads to a simplified, weighted variational bound objective for diffusion models (Section 3.4). Ultimately, our model design is justified by simplicity and empirical results (Section 4). Our discussion is categorized by the terms of Eq. (5).\n", "\n"], "children": [{"3.1 Forward process and LT": {"content": ["\n", "We ignore the fact that the forward process variances \u03b2t are learnable by reparameterization and instead fix them to constants (see Section 4 for details). Thus, in our implementation, the approximate posterior q has no learnable parameters, so LT is a constant during training and can be ignored.\n", "\n"], "children": []}}, {"3.2 Reverse process and L1:T \u22121": {"content": ["\n", "Now we discuss our choices in p\u03b8(xt\u22121|xt) = N (xt\u22121; \u00b5\u03b8 (xt, t), \u03a3\u03b8(xt, t)) for 1 < t \u2264 T. First, we set \u03a3\u03b8(xt, t) = \u03c3 2 t I to untrained time dependent constants. Experimentally, both \u03c3 2 t = \u03b2t and \u03c3 2 t = \u03b2\u02dc t = 1\u2212\u03b1\u00aft\u22121 1\u2212\u03b1\u00aft \u03b2t had similar results. The first choice is optimal for x0 \u223c N (0, I), and the second is optimal for x0 deterministically set to one point. These are the two extreme choices corresponding to upper and lower bounds on reverse process entropy for data with coordinatewise unit variance [53].\n", "\n", "Second, to represent the mean \u00b5\u03b8 (xt, t), we propose a specific parameterization motivated by the following analysis of Lt. With p\u03b8(xt\u22121|xt) = N (xt\u22121; \u00b5\u03b8 (xt, t), \u03c32 t I), we can write:\n", "\n", "$$L_{t-1}=\\mathbb{E}_{q}\\left[\\frac{1}{2\\sigma_{t}^{2}}\\|\\hat{\\mathbf{\\mu}}_{t}(\\mathbf{x}_{t},\\mathbf{x}_{0})-\\mathbf{\\mu}_{\\theta}(\\mathbf{x}_{t},t)\\|^{2}\\right]+C\\tag{8}$$\n", "\n", "where C is a constant that does not depend on \u03b8. So, we see that the most straightforward parameterization of \u00b5\u03b8 is a model that predicts \u00b5\u02dct , the forward process posterior mean. However, we can expand Eq. (8) further by reparameterizing Eq. (4) as xt(x0, ) = \u221a \u03b1\u00aftx0 + \u221a 1 \u2212 \u03b1\u00aft for \u223c N (0, I) and applying the forward process posterior formula (7):\n", "\n", "$$L_{t-1}-C=\\mathbb{E}_{\\alpha_{0},\\epsilon}\\left[\\frac{1}{2\\sigma_{t}^{2}}\\left\\|\\hat{\\mu}_{t}\\bigg{(}\\mathbf{x}_{t}(\\mathbf{x}_{0},\\epsilon),\\frac{1}{\\sqrt{\\alpha_{t}}}(\\mathbf{x}_{t}(\\mathbf{x}_{0},\\epsilon)-\\sqrt{1-\\alpha_{t}}\\epsilon)\\bigg{)}-\\mu_{g}(\\mathbf{x}_{t}(\\mathbf{x}_{0},\\epsilon),t)\\right\\|^{2}\\right]\\tag{9}$$\n", "\n", "$$=\\mathbb{E}_{\\mathbf{x}_{0},\\epsilon}\\left[\\frac{1}{2\\sigma_{t}^{2}}\\left\\|\\frac{1}{\\sqrt{\\alpha_{t}}}\\left(\\mathbf{x}_{t}(\\mathbf{x}_{0},\\epsilon)-\\frac{\\beta_{t}}{\\sqrt{1-\\alpha_{t}}}\\epsilon\\right)-\\boldsymbol{\\mu}_{\\theta}(\\mathbf{x}_{t}(\\mathbf{x}_{0},\\epsilon),t)\\right\\|^{2}\\right]\\tag{10}$$\n", "\n"], "children": []}}]}}]}}